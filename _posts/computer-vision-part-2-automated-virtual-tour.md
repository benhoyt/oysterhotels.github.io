---
title: "Automated virtual tour"
author: Tuan
layout: post
permalink: /computer-vision-part-2-automated-virtual-tour/
disqus_page_identifier: computer-vision-part-2-automated-virtual-tour
published: false
---

![HDR Panorama](/public/images/cv2-cover.png)

Welcome back to the 2nd of the 3-part Computer Vision series at Oyster.com. If you have not seen our 1st part of the series, we would recommend you [check it out](http://tech.oyster.com/computer-vision-part-1-hdr-panorama), in that part we show how HDR (High Dynamic Range) panoramas are done at Oyster.com (including some comparisons of our panoramas vs Google.com panoramas).

In this part, we will share the behind-the-scene work of our recently added feature [Virtual walkthrough](https://www.oyster.com/french-polynesia/hotels/intercontinental-bora-bora-le-moana-resort/all-tours/pool--v35274/), or short as walkthrough.

We will first give an introduction of walkthrough, and why we need to build our own framework for this purpose. We will then jump right into the details of our computer vision system that generates walkthroughs from sets of panoramas. Lastly we will show some examples of walkthroughs on our sites for different indoor and outdoor scenarios.

## Generating walkthroughs from 2D panoramas
Walkthrough is a set of connected panoramas where users can navigate from panorama to another, this type of feature provides more interactive experience for users at remote sites. Walkthroughs can be generated by different approaches. One common approach is to use depth information to reconstruct the 3D scene for each panorama spot, like [Matterport](https://matterport.com), this provides seamless transition from scene to scene but it comes with high cost in purchasing their own special device and model hosting, in additional to its quality is not comparable to standard DLSR cameras. The more economic and more popular approach is the one that uses 2D panoramas to build walkthroughs based on popular 360 image viewers such as [Krpano](http://krpano.com/examples/vtour) to connect panoramas together. We build our framework to generate walkthroughs from sets of panoramas also based on Krpano, but in order to scale this approach for our huge database of hotels, we leverage Computer Vision techniques to automate the whole process from panoramas to completed walkthroughs.

## Automating walkthrough process
Our process starts with a set of HDR panoramas as input, finds the panoramas that are connected, estimates the relationship of those connected panoramas, and produces links that are integratable into a Krpano virtual tour's template project. For a set of n panoramas, we carry out n * (n - 1) / 2 pano-pano matchings. The whole process for a pano-pano matching has 3 main steps, namely disintegrating local views, matching and estimating panorama links, and integrating local links.

Set of equirectangular panoramas as input
![Set of equirectangular panoramas as input](/public/images/cv2-pano-set.png)

### Disintegrating local views - Pano-pano matching
A 360 panorama is a representation of a sphere which center is at the camera location. Two 360 panoramas are connected when the camera location of one panorama is inside the scene of the other panorama. In order to find out if two panoramas are connected, we need to look for camera position on one panorama in the other panorama. The original equirectangular format of input panorama can be divided into 6 non-overlapping local views representing the Up, Down, Left, Right, Front, and Back side of the cube covering the 360 sphere of the panorama scene. This division enables us to use epipolar geometry of two image planes sharing overlapping views to find epipoles, which are camera positions in our case.

Spliting equirectangular panorama into 6 planar sides using krpano
```python
krpanotools64.exe makepano image.tif normal.config
```

![Spliting equirectangular panorama into 6 planar sides](/public/images/cv2-pano-disintegrating.png)

Epipolar geometry is commonly used when there are multiple image views sharing overlaps, as long as enough (7+) corresponding points (points that appear in both image views) are detected in two scenes, a fundamental matrix can be found to describe the intrinsic projective geometry between two views. [OpenCV](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html) provides all convenient methods to find fundamental matrix and epipoles from corresponding points, and those are what we are gonna use for our purpose.

Epipolar geometry - positions of epipoles e and e' are the camera locations of the 2 views in each other's image
![Epipolar geometry](/public/images/cv2-pano-epipolar.png)

### Matching and finding pano links - local view matching

In this step, we will be finding corresponding points and then camera positions for each pano view. Corresponding points are the pairs of points, one from each image view and both display the same point in real-world. There are 3 substeps in this process, we first find a set of points of interest in each image (those points of interest normally represent corners, edges or discriminative patterns in each image view), then we use the region around each point to estimate if two points are displaying the same real-world point, then corresponding points are used to estimate the camera locations.

Epipolar geometry is commonly used to describe the geometry of multiple pin-hole cameras capturing the same 3D scene. It is also essential for our problem because our problem of generating virtual tour with different set of images taken at different locations can be formulated as an epipolar scenario where a scene is projected onto two pin-hole cameras. By aligning our problem into epipolar geometry, we can then be able to make use of some of its constraints to solve our problem, which is finding camera position.

Given two panoramas, which we can call Left and Right, the problem of creating the virtual tour for these 2 panoramas becomes finding the spherical coordinate of camera Left in panorama Right, and coordinate of camera Right in panorama Left. By projecting our spherical coordinates (defined by horizontal angular ath and vertical angular atv) into planar coordinates (horizontal value x and horizontal value y) (where the reprojection from planar to spherical is given by: ath = (x/width - 0.5) * hfov and atv = (y/height - 0.5) * vfov, and hfov = 360 and vfov = 180 for our complete panorama), resulting in 6 slices (Left, Right, Front, Back, Up, Down), we can look for the location of camera Left in all 6 slices of camera Right and vice versa the location of camera Right in all 6 slices of camera Left. Therefore, the original problem of finding camera location for each pair of panoramas becomes a search for camera location in all combinations of panorama slices, which is n * (n - 1) / 2 matchings.

For each image-image matching, the camera location is the camera center of the pin-hole camera model, and a pair of images forms an epipolar geometry. If we call these two images Left and Right, and camera centers of the two images form a line from image Left to image Right and intersects each image at a point called Epipole. The Epipole on image Left is the pixel coordinate (xL, yL) of camera center Right on image Left, and the Epipole on image Right is the pixel coordinate (xR, yR) of camera center Left on image Right. 

In pin-hole camera model, a pixel coordinate on an image represents a set of points lying on the ray light from camera center toward that point in 3D (and goes on to infinity). With another camera viewing the same scene, we can see that line, or in other words, a point in one camera is transferable into a line in another camera in epipolar geometry, this line is corresponding line. All the corresponding lines have a common property, they all go through the Epipole, that is, given all points on image Left, we can project all corresponding lines on image Right, and all these corresponding lines intersect at Epipole Right on image Right, and similarly for Epipole Left on image Left. In order to find corresponding lines from image points in pixel coordinate, we need first to find the fundamental matrix of the epipolar geometry. This fundamental matrix is a rank-2 3x3 matrix that represents the relative pose (translation + rotation) of image Left and right Right (or vice versa) as well as the intrinsic parameters of two camera. It has 7 parameters, 2 for each Epipole, and 3 for the homography that relates the two image planes. The convenient property of fundamental matrix is that it can be calculated from sufficient corresponding points. Corresponding points are pixel points appear on two images that are pointing to the same 3D real-world point.

With all those theories established, our problem of connecting panoramas into a virtual walkthrough now comes down to finding corresponding points on each slice image pair of the two panoramas. For this task, we resort to feature matching, which is a robust approach for dynamic views. The matching consists of three main steps, feature detection and feature matching and feature pruning. 




#### Feature detection
Feature detection is the process of running pre-defined feature filters on an image to discover features that are discriminative and view invariant (for example point at corners or edges where ). OpenCV has implementation for a collection of robust local features such as FAST, STAR, SIFT, or SURF (please check out [OpenCV's documentation](http://docs.opencv.org/3.1.0/db/d27/tutorial_py_table_of_contents_feature2d.html#gsc.tab=0) for more available feature detectors)

An example of how SIFT can be used for detecting local features is (Note: Since SIFT and SURF are patented feature, you should use other free features provided by OpenCV to avoid license fee)

```python
detector = cv2.xfeatures2d.SIFT_create(nfeatures=2000, nOctaveLayers=3, 
    contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)
kp, des = detector.detectAndCompute(self.image, self.mask)
```
In the code above, there are 2 paramas that are quite important, nfeatures and contrastThreshold, reducing contrast threshold or increasing number of maximum features will give us more features, and vice versa. Those values should be chosen based on the nature of image data that we are dealing with, and the focus of our detection process. In our case, the distance between camera locations in real-world coordinate is unknown, which could be too far so a more appropriate design is to extract as many features as we can at detection phase, then in feature matching and pruning phase we will filter out irrelevant features. More practical decisions like this will be discussed later in our last section, along with our coarse-to-fine approach to efficiently extract features at constrained processing time.

The following figure shows the result of our feature detection process on two images, one from each panorama, local features are drawn in different colors, and as we can see they are mostly detected on corners and edges, and some features seem to be detected on both paranomas, those are the corresponding points that we are looking for. 

![Local features detected from the two images](/public/images/cv2-pano-keypoint-detection.png) 

#### Feature matching 
At each location where the feature is detected, a set of attributes are extracted to define that feature, they are called feature descriptors, some of the most common feature descriptors implemented in OpenCV are SIFT, SURF, HOG, BRIEF, BRISK, again please refer to [OpenCV's documentation](http://docs.opencv.org/3.1.0/db/d27/tutorial_py_table_of_contents_feature2d.html#gsc.tab=0) for more available feature descriptors. Those descriptors can be seen as a normalized (orientation-wise) vectorized aggregation (spatial-wise) of primitive filter response (e.g. SIFT descriptor gives a 128-dimensional vector aggregated from 4 x 4 location bins in left-right top-down spatial order, each bin is represented by accumulated gradients grouped in 8 orientation bins).

Featue matching is the process of finding the same set of features that appear in both images given feature descriptors (in forms for multidimensional vectors). Given ten of thousands of features being detected in each image, an efficient matching approach is to use kd-tree (k dimensional binary tree) to first index all features of one image and matching with features from the other image can then be done by traversing the indexed trees. In order to minimized false negatives in matching, knn (finding the nearest k matches for each feature) is also used. All those theories can be done with OpenCV API in 2 lines of code

```python
matcher = cv2.FlannBasedMatcher(dict(algorithm=FLANN_INDEX_KDTREE, trees=5), dict(checks=50))
matches = matcher.knnMatch(self.pano_image_from.des, self.pano_image_to.des, k=2)
```

In this context, number of trees, traversal checks, and number of k nearest neighbors are chosen based on accuracy-time tradeoffs in our particular applications. The following figure shows the result of this matching step, as we can see in this design we aim to reduce false negative matches so more matches are returned than needed, those matches will be cleaned up in the following feature pruning section.

![Feature matching results](/public/images/cv2-pano-keypoint-matching.png) 

#### Feature pruning
Feature pruning is a series of filters being applied onto matched features, to filter out features that have passed through our previous matching process (which was purely based on similarity in appearance) but are not the exact corresponding features. These wrongly matched features are actually very common, most of the time they are features on similar or the same objects (e.g. features on the brick wall do share the same appearance across the whole wall, features on 2 different corners of the board do have similar appearance, just a rotated version of each other), or features that are noise (those are normally random dots on clean backgrounds that appear so frequently that there are many matches of these, with similar matching distance).

A successful feature matching system is mostly determined by how well feature pruning is implemented, if it is too strict we might end up with insufficent matches, but if it is too loose that will not only increase our processing time but will return in incorrect matches for later phases.

Pruning features should be designed based on the nature of the data. For our problem, there are 4 main pruning filters that can be used, which are ratio filter, cross-match filter, orientation consistency filter and spatial consistency filter.

Ratio filter is designed to remove feature noise as mentioned above, the idea is, given all matches of a feature along with their similarity scores, a feature is determined as a feature noise when its best and second best match has too similar similarity scores. Again, if a feature can be matched with similar confidence to 2 different features, that feature is considered as a feature noise (e.g. a random dot on a wall could be matched with similar confidence with other random dots). This filter checks for the ratio of similarity score between the second best match and the best match, that ratio has to fall under a certain value for that feature and that match to be valid. This pruning technique was first proposed by David Lowe (the author of SIFT feature) using 0.7 as ratio threshold, but this type of filter could be used effectively with any features that have explicit similarity scores.

Cross-match filter is checking for mutual matching result of a pair of features, in other words, 2 features are considered to be correctly matched when each feature appears in the match list of the other feature. 

Orientation consistency filter is specific for our particular problem where there are no rotation in the transformation of one image into another, that is, given the cameras are placed and locked on tripod when taking the photo, that results in images having same upright orientation wherever the tripod is placed. This filter checks for the dominant orientation of the feature and its match to see if they are close. This filter can be applied on any feature detector that calculates dominant orientation.

Spatial consistency filter is similar to orientation consistency, because image orientation is preserved between different shots, relative spatial relationship between filters are preserved, and this filter checks if a feature and its match keeps this relationship.

The following figure shows the result of feature pruning process after 4 filters have been applied, those features indicate a more clean and accurate match than the first matching set we obtained from pure appearance comparison.

![Feature pruning results](/public/images/cv2-pano-match-inliers.png) 

#### Epipole estimating

Once the set of miminal corresponding pairs is found, we can find fundamental matrix using RANSAC technique, which iteratively picks a subset inliners and project the model on the rest, the outliers.


## Practical implementation discussion
- Tweaking the number of slices
- Pair matching - local match using only 4 views instead of 6 views
- Number of features, coarse-to-fine structure
- Number of matches
- Free vs non-free (SIFT, SURF)



In this post, we have presented our approach to generating HDR panorama at large scale using available packages like DNGConverter, DCRAW, SNS-HDR, PTGui, and with the help from Computer Vision techniques with OpenCV. Please feel free to visit our website [Oyster](https://www.oyster.com) to see our rich collection of hotel panoramas all around the world. Also, please stay tuned for part 2 and 3 of this Computer Vision series, where we will show you how virtual tour can be generated (again fully automated at large scale) from a set of panoramas, and how smart features like mini-maps can be added to your tour to improve user experience.

### About the author:
Tuan Thi is a Senior Software Engineer in Computer Vision at [Oyster](https://www.oyster.com).com, part of Smarter Travel Media Group, at TripAdvisor. He finished his PhD in Computer Vision and Machine Learning in 2011.  Before joining TripAdvisor, he was a research engineer and computer vision scientist at Canon Research and Placemeter Ltd. with various international publications and patents in the field of local features, structured learning and deep learning.


