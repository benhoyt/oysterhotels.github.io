---
title: "Automated virtual tour"
author: Tuan
layout: post
permalink: /computer-vision-part-2-automated-virtual-tour/
disqus_page_identifier: computer-vision-part-2-automated-virtual-tour
published: false
---

![HDR Panorama](/public/images/cv2-cover.png)

Welcome back to 2nd part of the 3-part Computer Vision series at Oyster.com. If you have not seen our 1st part of the series, we would recommend you [check it out](http://tech.oyster.com/computer-vision-part-1-hdr-panorama), in that part we show how HDR (High Dynamic Range) panoramas are done at Oyster.com (including some comparisons of our panoramas vs Google.com panoramas).

In this part, we will share the behind-the-scene work of our recently added feature [Virtual walkthrough](https://www.oyster.com/french-polynesia/hotels/intercontinental-bora-bora-le-moana-resort/all-tours/pool--v35274/), or short as walkthrough.

We will first give an introduction of walkthrough, and why we need to build our own framework for this purpose. We will then jump right into the details of our computer vision system that generates walkthroughs from sets of panoramas. Lastly we will show some examples of walkthroughs on our sites for different indoor and outdoor scenarios.

## Generating walkthroughs from 2D panoramas
Walkthrough is a set of connected panoramas where users can navigate from panorama to another, this type of feature provides more interactive experience for users at remote sites. Walkthroughs can be generated by different approaches. One common approach is to use depth information to reconstruct the 3D scene for each panorama spot, like [Matterport](https://matterport.com), this provides seamless transition from scene to scene but it comes with high cost in purchasing their own special device and model hosting, in additional to its quality is not comparable to standard DLSR cameras. The more economic and more popular approach is the one that uses 2D panoramas to build walkthroughs based on popular 360 image viewers such as [Krpano](http://krpano.com/examples/vtour) to connect panoramas together. We build our framework to generate walkthroughs from sets of panoramas also based on Krpano, but in order to scale this approach for our huge database of hotels, we leverage Computer Vision techniques to automate the whole process from panoramas to completed walkthroughs.

## Automating walkthrough process
Our process starts with a set of HDR panoramas as input, finds the panoramas that are connected, estimates the relationship of those connected panoramas, and produces links that are integratable into a Krpano virtual tour's template project. For a set of n panoramas, we carry out n * (n - 1) / 2 pano-pano matchings. The whole process for a pano-pano matching has 3 main steps, namely disintegrating local views, matching and estimating panorama links, and integrating local links.

![Set of equirectangular panoramas as input](/public/images/cv2-pano-set.png)

### Disintegrating local views
A 360 panorama is a representation of a sphere which center is at the camera location. Two 360 panoramas are connected when the camera location of one panorama is inside the scene of the other panorama. In order to find out if two panoramas are connected, we need to look for camera position on one panorama in the other panorama. The original equirectangular format of input panorama can be divided into 6 non-overlapping local views representing the Up, Down, Left, Right, Front, and Back side of the cube covering the 360 sphere of the panorama scene. This division enables us to use epipolar geometry of two image planes sharing overlapping views to find epipoles, which are camera positions in our case.

![Spliting equirectangular panorama into 6 planar sides](/public/images/cv2-pano-disintegrating.png)

Epipolar geometry is commonly used when there are multiple image views sharing overlaps, as long as enough (7+) corresponding points (points that appear in both image views) are detected in two scenes, a fundamental matrix can be found to describe the intrinsic projective geometry between two views. [OpenCV](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html) provides all convenient methods to find fundamental matrix and epipoles from corresponding points, and those are what we are gonna use for our purpose.

![Epipolar geometry](/public/images/cv2-pano-epipolar.png)

### Matching and finding pano links

In this step, we will be finding corresponding points and then camera positions for each pano view. There are 3 substeps: feature detection, feature matching and epipole estimating.

#### Feature detection
This is straightforward using OpenCV, the idea is to find all interest points in the two local views of the pano pairs. OpenCV provides a great set of robust local features like ORB, SIFT.

#### Feature matching and pruning
Featue matching is also straightforward with OpenCV API, the most common approach is FLANN-based matching, which first index features using kd-tree then match using nearest neighbors

Feature pruning is the actual step that brings robustness to our problem. We do not want to miss out any important features, so we include as many features as we can in our previous step, but at the same time, we want to narrow down to only important features using pruning techniques.
There are 3 main pruning filters that we apply in our approach, first by ratio check, recommended by David Lowe's used for his SIFT feature, the second is crosscheck, meaning one match is valid when there exists the match from the other view to this view too, the third pruning filter is the check for spatial relativeness, and main angle difference (does ORB have this?)

#### Epipole estimating

Once the set of miminal corresponding pairs is found, we can find fundamental matrix using RANSAC technique, which iteratively picks a subset inliners and project the model on the rest, the outliers.


## Practical implementation discussion
- Tweaking the number of slices
- Pair matching
- Number of features
- Number of matches



In this post, we have presented our approach to generating HDR panorama at large scale using available packages like DNGConverter, DCRAW, SNS-HDR, PTGui, and with the help from Computer Vision techniques with OpenCV. Please feel free to visit our website [Oyster](https://www.oyster.com) to see our rich collection of hotel panoramas all around the world. Also, please stay tuned for part 2 and 3 of this Computer Vision series, where we will show you how virtual tour can be generated (again fully automated at large scale) from a set of panoramas, and how smart features like mini-maps can be added to your tour to improve user experience.

### About the author:
Tuan Thi is a Senior Software Engineer in Computer Vision at [Oyster](https://www.oyster.com).com, part of Smarter Travel Media Group, at TripAdvisor. He finished his PhD in Computer Vision and Machine Learning in 2011.  Before joining TripAdvisor, he was a research engineer and computer vision scientist at Canon Research and Placemeter Ltd. with various international publications and patents in the field of local features, structured learning and deep learning.


